version: '3.7'
# Settings and configurations that are common for all containers

services:
    everythingdata:
      networks:
        - traefik
      container_name: everythingdata
      build: .
      user: root
      build: .
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]
      environment:
       - "JUPYTER_PASSWORD"
       - "TRANSLATOR"
       - "TEST"
       - "TOKEN"
       - "DATAPATH"
       - "MIN_SENTENCE_SIZE"
       - "CONFIG"
       - "CONFIGFILE"
       - "MODEL"
       - "LLAMAMODEL"
       - "DIDAPI"
       - "NERAPI"
       - "LLMRAMAPI"
       - "LLMAPI"
       - "GRAPHDIR"
       - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
       - "OLLAMA_API"
       - "DEBUG=1234567801"
      ports:
        - "8008:8000"
        - "8009:8888"
      command: uvicorn main:app --host 0.0.0.0 --port 8000
      volumes:
        - ${DATADIR}:/data
        - ${CONFIG}:/app/config/prompts.ini
        - ${MODELS}:/root/.cache/huggingface
        - ${GRAPHDIR}:/app/config
        - ./app/app.py:/app/main.py
        - ./app/llmframe.py:/app/llmframe.py
        - ./app/localconfig.py:/app/localconfig.py
        - ./app/utils.py:/app/utils.py

    llama:
      volumes:
        - ./ollama-docker/ollama/ollama:/root/.ollama
      networks:
        - traefik
      container_name: ollama
      pull_policy: always
      tty: true
      restart: unless-stopped
      image: ollama/ollama:latest
      ports:
        - 11434:11434
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                capabilities: [gpu]
                device_ids: ['1']

networks:
  traefik:
    external: true
