version: '3.7'
# Settings and configurations that are common for all containers

services:
    everythingdata:
      container_name: everythingdata
      user: root
      image: dansknaw/everythingdata:devel-${cudaversion}
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: ${gpucount}
                capabilities: [gpu]
      environment:
       - "TEST"
       - "TOKEN"
       - "DATAPATH"
       - "MIN_SENTENCE_SIZE"
       - "CONFIG"
       - "CONFIGFILE"
       - "MODEL"
       - "LLAMAMODEL"
       - "DIDAPI"
       - "NERAPI"
       - "LLMRAMAPI"
       - "LLMAPI"
       - "GRAPHDIR"
       - "TRANSLATOR"
       - "MAX_WORKERS"
       - "WEB_CONCURRENCY"
       - "--n_samples=1"
       - "PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512"
       - "OLLAMA_API"
       - "DEBUG=1234567801"
      ports:
        - "${llmport}:8000"
      command: uvicorn main:app --host 0.0.0.0 --port 8000
      volumes:
        - ${DATADIR}:/data
        - ${CONFIG}:/app/config/prompts.ini
        - ${MODELS}:/root/.cache/huggingface
        - ./app/app.py:/app/main.py
        - ./app/llmframe.py:/app/llmframe.py
        - ./app/localconfig.py:/app/localconfig.py
        - ./config/prompts.ini:/app/config/prompts.ini
        - ./app/utils.py:/app/utils.py

    llama:
      volumes:
        - ./ollama-docker/ollama/ollama:/root/.ollama
      container_name: ollama
      pull_policy: always
      tty: true
      restart: unless-stopped
      image: ollama/ollama:latest
      ports:
        - 11434:11434
      deploy:
        resources:
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]

